CMU Pocket Sphinx Engine
============================================================================

This directory contains the CMU Pocket Sphinx engine components.

The engine uses Pocket Sphinx's support for *JSpeech Grammar Format* (JSGF). JSGF is similar to the grammar format that *NatLink* and *Dragon NaturallySpeaking* use, with the notable exception of built-in functions/rules such as `<Dictation()>`.

The engine uses the [dragonfly2jsgf](../../../dragonfly2jsgf/) and *pyjsgf* packages to handle the differences in the grammar formats, *sphinxwrapper* to interact with Pocket Sphinx, and *pyaudio* to stream audio to Pocket Sphinx from a microphone.


## To-do list
The following is a list of to-do items for the engine implementation in no particular order:
  - [X] dictation support with pauses
  - [X] use searches for each grammar instead of one large search
  - [ ] add support for command chaining + `Dictation` elements
  - [ ] full action support using Aenea in some way
  - [ ] dynamically add unknown words to the dictionary and language model using the decoder `add_word` method, somehow getting a phone string for the second parameter, perhaps using [lextool](http://www.speech.cs.cmu.edu/tools/lextool.html) or something similar. The underlying `ps_add_word` C function is documented in the include file [here](https://github.com/cmusphinx/pocketsphinx/blob/master/include/pocketsphinx.h#L245).
  - [ ] for better performance, use Python's `multiprocessing` package and multiple PS decoders to call `<decoder>.batch_process` in parallel.
  - [ ] properly document the public methods for the Sphinx engine with the Sphinx documentation that dragonfly uses

Testing and examples
----------------------------------------------------------------------------
There are Python unit tests for this engine [here](../../test/test_engine_sphinx.py) and an example grammar module [here](../../examples/sphinx_engine_example.py). The engine grammar module loader is [here](../../examples/sphinx_module_loader.py).


Cross-platform Engine
----------------------------------------------------------------------------

Pocket Sphinx runs on most platforms, including on architectures other than x86, so it only makes sense that this version of dragonfly should work on non-Windows systems like macOS as well as Linux distributions. To this effect, I've made an effort to mock Windows-only functionality for non-Windows platforms for the time being to allow the engine components to work correctly regardless of the platform.

The *pyaudio* package used by the engine works on Windows, macOS and Linux systems by unifying the sound APIs. It is the Python package for [portaudio](http://www.portaudio.com/). So fortunately, no work there!

Using dragonfly with a non-Windows operating system can already be done with [Aenea](https://github.com/dictation-toolbox/aenea) using the existing *NatLink* engine. Aenea communicates with a separate Windows system running *NatLink* and *DNS* over a network connection and has server support for Linux (using X11), macOS, and Windows. I think it should be possible to make this engine work with aenea. There would some required changes to the client `aenea` package on other operating systems if you want to run it natively through a loopback interface, for example.

Another approach is to use aenea's proxy contexts and actions in dragonfly directly, instead of the mocked ones. I'm not sure which is the best approach here.


Engine Configuration
----------------------------------------------------------------------------

There is a default configuration module for the Sphinx engine [here](config.py).

This could be useful if you need to, for example, use different models or a different pronunciation dictionary for Pocket Sphinx. You can change the decoder configuration in the module.

The `config` property of the engine class can be used to set the configuration module/object you want to use. If you change the configuration while the engine is connected, the decoder configuration used will not take effect until you restart the engine using `disconnect()` and then `connect()` again. Same with the PyAudio stream configuration; you need to disconnect and then do `recognise_forever()` again.


Improving Speech Recognition Accuracy
----------------------------------------------------------------------------

The Sphinx engines can have some trouble recognising what was said accurately. To remedy this, you may need to adapt the acoustic model that Pocket Sphinx is using. This is similar to training DNS. There is information on how to do that [here](https://cmusphinx.github.io/wiki/tutorialadapt/) and a good video on it [here](https://www.youtube.com/watch?v=IAHH6-t9jK0). Adapting an acoustic model is currently a rather involved process, although it could be improved with some scripting.

Adapting your model might not be necessary; there might be other issues with your setup. There's more information on tuning the recognition accuracy [here](https://cmusphinx.github.io/wiki/tutorialtuning/).


Limitations
----------------------------------------------------------------------------

There are a few limitations with this engine, most notably with the CMU Sphinx spoken language support and dragonfly's 'Dictation' functionality. That said, most of the grammar functionality will work perfectly.


### Dictation

Unfortunately, the 'Dictation' support that DNS and WSR provide is difficult to replicate with CMU Sphinx engines. They don't support speaking grammar rules that include `Dictation` elements, although they will work for this engine, you'll just have to pause between speaking the grammar and dictation parts of rules that use `Dictation` extras. There is timeout configuration for this, see [the default engine config](config.py).

'Dictation' output also won't have words properly capitalised as they are when using DNS, all words will be in lowercase. Additionally, punctuation words like "comma" or "apostrophe" won't have special output, although such functionality can be added either through grammars or processing of the dictation output.


### Spoken Language Support

There are only a handful of languages with working language models and dictionaries available [here](https://sourceforge.net/projects/cmusphinx/files/Acoustic%20and%20Language%20Models/), although it is possible to build your own language model [using lmtool](http://www.speech.cs.cmu.edu/tools/lmtool-new.html) or pronunciation dictionary [using lextool](http://www.speech.cs.cmu.edu/tools/lextool.html). There are other tools out there as well. See more on building a language model [here](https://cmusphinx.github.io/wiki/tutoriallm/).

You may get errors if your grammars use words that aren't in Pocket Sphinx's loaded pronunciation dictionary and/or language model. This can happen if you've misspelled words, if the words aren't written in the same language that the dictionary and language model are for (e.g. US English), or if the words just aren't in the loaded dictionary or language model.

There are Pocket Sphinx functions for adding words to the dictionary and language model if you need to. I think doing this dynamically in the engine would be useful, perhaps with a configuration option.


### Dragonfly Lists and Dictionaries

Dragonfly lists and dictionaries function as normal rules for the Pocket Sphinx engine. So on updating a dragonfly list or dictionary, the grammar they are part of will be reloaded. This is because there is, unfortunately, no JSGF equivalent for lists.


### Text-to-speech

CMU Sphinx recognition engines do not come with text-to-speech functionality. I would recommend looking at the [Jasper project's implementations](https://github.com/jasperproject/jasper-client/blob/master/client/tts.py) for using popular text-to-speech engines if you really need it in dragonfly. You would need to implement the `speak` method in the `SphinxEngine` class.
